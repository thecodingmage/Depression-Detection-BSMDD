{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHOGJnJvT2OE",
        "outputId": "deadeff2-b3b2-44f5-e879-903010069af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Libraries imported, Drive mounted, and dataset loaded successfully.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21910 entries, 0 to 21909\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   text_bengali   21910 non-null  object\n",
            " 1   text_banglish  21910 non-null  object\n",
            " 2   label          21910 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 513.6+ KB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# For Keras model building\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# For data splitting and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Depression detection dataset/BSMDD_main.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop any rows where the text is missing and reset index\n",
        "df.dropna(subset=['text_banglish'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"Libraries imported, Drive mounted, and dataset loaded successfully.\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define the text preprocessing function ---\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # Convert to lowercase and ensure it's a string\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation, numbers, etc.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# --- 2. Apply the function to create a 'cleaned_text' column ---\n",
        "df['cleaned_text'] = df['text_banglish'].apply(preprocess_text)\n",
        "\n",
        "# --- 3. Safeguard: Remove any rows that became empty after cleaning ---\n",
        "original_rows = len(df)\n",
        "df = df[df['cleaned_text'] != ''].reset_index(drop=True)\n",
        "if original_rows > len(df):\n",
        "    print(f\"Removed {original_rows - len(df)} rows that were empty after preprocessing.\")\n",
        "\n",
        "# --- 4. Display the result to verify ---\n",
        "print(\"DataFrame after preprocessing:\")\n",
        "print(df[['text_banglish', 'cleaned_text', 'label']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65x2_uG-T8jr",
        "outputId": "b288d0cc-f4d3-4311-962a-791e411cb18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 2 rows that were empty after preprocessing.\n",
            "DataFrame after preprocessing:\n",
            "                                       text_banglish  \\\n",
            "0  manasika sharirikabhabe asustha klanta puro ji...   \n",
            "1  daya sathe thakuna atyanta dirgha apanake pada...   \n",
            "2  janatama sathe bhula loka kharapa jibana katiy...   \n",
            "3  anetibha imreji spikarera anusarana biraktikar...   \n",
            "4  anetibha imreji spikarera anusarana biraktikar...   \n",
            "\n",
            "                                        cleaned_text  label  \n",
            "0  manasika sharirikabhabe asustha klanta puro ji...      1  \n",
            "1  daya sathe thakuna atyanta dirgha apanake pada...      1  \n",
            "2  janatama sathe bhula loka kharapa jibana katiy...      1  \n",
            "3  anetibha imreji spikarera anusarana biraktikar...      1  \n",
            "4  anetibha imreji spikarera anusarana biraktikar...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Download the GloVe Embeddings ---\n",
        "# We'll use the 100-dimensional version trained on 6 billion tokens.\n",
        "print(\"Downloading GloVe embeddings... (This is an 822MB file and may take several minutes)\")\n",
        "!wget --quiet http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "print(\"\\nUnzipping the file...\")\n",
        "!unzip -q glove.6B.zip # The -q flag makes the output less verbose\n",
        "\n",
        "\n",
        "# --- 2. Parse the GloVe File and Load into a Dictionary ---\n",
        "# We create a dictionary that maps words (strings) to their embedding vectors (numpy arrays).\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "embedding_dim = 100 # This must match the GloVe file we are using (e.g., 100d)\n",
        "glove_embeddings = {}\n",
        "\n",
        "print(f\"\\nLoading word vectors from '{glove_file}' into memory...\")\n",
        "with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings[word] = vector\n",
        "\n",
        "print(f\"Successfully loaded {len(glove_embeddings):,} word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEJBPNBoURIr",
        "outputId": "e32f345d-cd1d-4f4a-a185-dd272ae4ee46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings... (This is an 822MB file and may take several minutes)\n",
            "\n",
            "Unzipping the file...\n",
            "\n",
            "Loading word vectors from 'glove.6B.100d.txt' into memory...\n",
            "Successfully loaded 400,000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Use Keras Tokenizer to convert texts to integer sequences ---\n",
        "texts = df['cleaned_text'].tolist()\n",
        "labels = df['label'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "keras_vocab_size = len(word_index) + 1\n",
        "print(f\"Vocabulary size for Keras Tokenizer (our dataset): {keras_vocab_size}\")\n",
        "\n",
        "\n",
        "# --- 2. Pad sequences to a uniform length ---\n",
        "lengths = [len(s) for s in sequences]\n",
        "maxlen = int(np.percentile(lengths, 95))\n",
        "print(f\"Padding sequences to a max length of: {maxlen}\")\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
        "y = labels\n",
        "\n",
        "\n",
        "# --- 3. Create the embedding matrix from the GloVe model ---\n",
        "# This matrix will be used as the initial weights for the Keras Embedding layer.\n",
        "hits = 0\n",
        "misses = 0\n",
        "embedding_matrix = np.zeros((keras_vocab_size, embedding_dim)) # embedding_dim is 100\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    # Get the vector for the word from our loaded GloVe embeddings\n",
        "    embedding_vector = glove_embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the GloVe vocabulary will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "\n",
        "print(f\"\\nShape of the embedding matrix: {embedding_matrix.shape}\")\n",
        "print(f\"Converted {hits} words from our vocabulary ({misses} misses).\")\n",
        "\n",
        "\n",
        "# --- 4. Split data into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "print(f\"\\nTraining data shape (X_train): {X_train.shape}\")\n",
        "print(f\"Testing data shape (X_test): {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVnpL9TwUHlS",
        "outputId": "8266e68d-13d3-4e6c-cd01-2787a65771f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size for Keras Tokenizer (our dataset): 56827\n",
            "Padding sequences to a max length of: 192\n",
            "\n",
            "Shape of the embedding matrix: (56827, 100)\n",
            "Converted 3578 words from our vocabulary (53248 misses).\n",
            "\n",
            "Training data shape (X_train): (17526, 192)\n",
            "Testing data shape (X_test): (4382, 192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Build the GRU Model ---\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer (loaded with our GloVe-based weights)\n",
        "# We set trainable=False because the embeddings are already pre-trained.\n",
        "model.add(Embedding(input_dim=keras_vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxlen,\n",
        "                    trainable=False))\n",
        "\n",
        "# GRU Layer\n",
        "model.add(GRU(units=64))\n",
        "\n",
        "# Dropout for regularization to help prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# A standard fully-connected Dense layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Final Output Layer for binary classification (using sigmoid for probabilities)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# --- 2. Compile the Model ---\n",
        "# We configure the model for training with an optimizer, loss function, and metrics.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "\n",
        "# --- 3. Print Model Summary ---\n",
        "# This summary is useful for understanding the model's structure and parameter count.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "XsLTOSILVODi",
        "outputId": "f3bee942-e377-499b-e93d-4bacb090389c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m5,682,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Set up Training Parameters ---\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "# Add a callback for Early Stopping to prevent overfitting.\n",
        "# It will stop training if the validation loss doesn't improve for 3 consecutive epochs\n",
        "# and will restore the weights from the best epoch.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define number of epochs and batch size\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# --- 2. Train the Model and Measure Time ---\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test), # Evaluate on test data at the end of each epoch\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the training time\n",
        "training_time = end_time - start_time\n",
        "print(f\"\\nTraining finished in {training_time:.2f} seconds (approx {training_time/60:.1f} minutes).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBPKK_kBVZk4",
        "outputId": "410fa403-391d-447b-9cbc-1cb36972a58a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 139ms/step - accuracy: 0.4977 - loss: 0.6933 - precision: 0.4962 - recall: 0.4626 - val_accuracy: 0.5126 - val_loss: 0.6927 - val_precision: 0.6321 - val_recall: 0.0611\n",
            "Epoch 2/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 139ms/step - accuracy: 0.5018 - loss: 0.6915 - precision: 0.5031 - recall: 0.3506 - val_accuracy: 0.4952 - val_loss: 0.6933 - val_precision: 0.4977 - val_recall: 0.9795\n",
            "Epoch 3/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 136ms/step - accuracy: 0.5045 - loss: 0.6908 - precision: 0.5019 - recall: 0.3928 - val_accuracy: 0.5100 - val_loss: 0.6931 - val_precision: 0.6286 - val_recall: 0.0502\n",
            "Epoch 4/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 140ms/step - accuracy: 0.5174 - loss: 0.6877 - precision: 0.5396 - recall: 0.3432 - val_accuracy: 0.5100 - val_loss: 0.6938 - val_precision: 0.6301 - val_recall: 0.0497\n",
            "\n",
            "Training finished in 321.40 seconds (approx 5.4 minutes).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- 1. Performance Evaluation on Test Set ---\n",
        "print(\"--- Final Performance Evaluation ---\")\n",
        "\n",
        "# Get model's prediction probabilities on the test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary class labels (0 or 1) using a 0.5 threshold\n",
        "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate and print the final metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "# Note: AUC is calculated on the prediction probabilities, not the binary predictions\n",
        "auc_roc = roc_auc_score(y_test, y_pred_probs)\n",
        "\n",
        "print(f\"\\nOverall Test Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"AUC-ROC:   {auc_roc:.4f}\\n\")\n",
        "\n",
        "# Display a detailed classification report including precision and recall\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Depressed (0)', 'Depressed (1)']))\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "# Structure: [[True Negative, False Positive], [False Negative, True Positive]]\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Practical Time Complexity Analysis ---\n",
        "print(\"--- Time Complexity ---\")\n",
        "\n",
        "# We already have the training time from the previous step\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds (approx. {training_time/60:.1f} minutes)\")\n",
        "\n",
        "# Measure Inference Time on the entire test set\n",
        "inference_start_time = time.time()\n",
        "_ = model.predict(X_test)\n",
        "inference_end_time = time.time()\n",
        "\n",
        "total_inference_time = inference_end_time - inference_start_time\n",
        "avg_inference_time_per_sample = total_inference_time / len(X_test)\n",
        "\n",
        "print(f\"Total Inference Time for {len(X_test)} samples: {total_inference_time:.4f} seconds\")\n",
        "print(f\"Average Inference Time per Sample: {avg_inference_time_per_sample * 1000:.4f} milliseconds\\n\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Practical Space Complexity Analysis ---\n",
        "print(\"--- Space Complexity ---\")\n",
        "\n",
        "# Number of total parameters in the model\n",
        "total_params = model.count_params()\n",
        "print(f\"Total Model Parameters: {total_params:,}\")\n",
        "\n",
        "# Save the model to a file to check its size on disk\n",
        "model_filename = \"depression_detection_gru_glove.keras\"\n",
        "model.save(model_filename)\n",
        "model_size_bytes = os.path.getsize(model_filename)\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Model Size on Disk: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjxOQSBQYUfm",
        "outputId": "ed208758-8b02-4213-8195-f50dc32514ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Final Performance Evaluation ---\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step\n",
            "\n",
            "Overall Test Metrics:\n",
            "Accuracy:  0.5126\n",
            "F1 Score:  0.1115\n",
            "AUC-ROC:   0.5139\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Not Depressed (0)       0.51      0.96      0.66      2190\n",
            "    Depressed (1)       0.63      0.06      0.11      2192\n",
            "\n",
            "         accuracy                           0.51      4382\n",
            "        macro avg       0.57      0.51      0.39      4382\n",
            "     weighted avg       0.57      0.51      0.39      4382\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2112   78]\n",
            " [2058  134]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Time Complexity ---\n",
            "Total Training Time: 321.40 seconds (approx. 5.4 minutes)\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step\n",
            "Total Inference Time for 4382 samples: 3.4846 seconds\n",
            "Average Inference Time per Sample: 0.7952 milliseconds\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Space Complexity ---\n",
            "Total Model Parameters: 5,716,685\n",
            "Model Size on Disk: 22.10 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "5Z9QBI9fYVBz",
        "outputId": "dafcffd1-c4bc-4070-ab06-941c82fcc95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m5,682,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m31,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,784,657\u001b[0m (22.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,784,657</span> (22.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,985\u001b[0m (132.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,985</span> (132.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m67,972\u001b[0m (265.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,972</span> (265.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "# --- 1. Save the Tokenizer to a file in the Colab environment ---\n",
        "# This is essential for preprocessing new data with this model later on.\n",
        "tokenizer_filename = 'tokenizer_glove.pickle'\n",
        "with open(tokenizer_filename, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"Keras Tokenizer saved to '{tokenizer_filename}'\")\n",
        "\n",
        "\n",
        "# --- 2. Copy the model artifacts to your Google Drive ---\n",
        "# Define the destination folder in your Google Drive\n",
        "destination_folder = '/content/drive/MyDrive/Depression detection dataset/saved_models/'\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# List of all the files we want to save\n",
        "files_to_copy = [\n",
        "    'depression_detection_gru_glove.keras', # Saved in the evaluation step\n",
        "    'tokenizer_glove.pickle'                # Just saved now\n",
        "]\n",
        "\n",
        "# Loop through the files and copy them to your Drive\n",
        "for filename in files_to_copy:\n",
        "  source_path = f'./{filename}'\n",
        "  destination_path = os.path.join(destination_folder, filename)\n",
        "  if os.path.exists(source_path):\n",
        "    !cp \"{source_path}\" \"{destination_path}\"\n",
        "    print(f\"Successfully copied '{filename}' to your Google Drive.\")\n",
        "  else:\n",
        "    print(f\"Warning: '{filename}' not found. Please ensure the previous cells were run.\")\n",
        "\n",
        "print(f\"\\nYour files are now safely stored in your Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mDknu7rYbLt",
        "outputId": "da67926a-a72d-4984-8828-00477a54a192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras Tokenizer saved to 'tokenizer_glove.pickle'\n",
            "Successfully copied 'depression_detection_gru_glove.keras' to your Google Drive.\n",
            "Successfully copied 'tokenizer_glove.pickle' to your Google Drive.\n",
            "\n",
            "Your files are now safely stored in your Google Drive!\n"
          ]
        }
      ]
    }
  ]
}