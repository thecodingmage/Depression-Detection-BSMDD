{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "dBoTh1I7Igbq",
        "outputId": "a4756e30-75ee-476c-e3c4-e90455cc8df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "ac3bba40a048497d8fc48cd9f551ba95"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# For Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# For Keras model building\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# For data splitting and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Depression detection dataset/BSMDD_main.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop any rows where the text is missing and reset index\n",
        "df.dropna(subset=['text_banglish'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"Libraries imported, Drive mounted, and dataset loaded successfully.\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuD8Y72HJbio",
        "outputId": "a7f2a72f-7062-483d-82e9-d7eaf98ac3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Libraries imported, Drive mounted, and dataset loaded successfully.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21910 entries, 0 to 21909\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   text_bengali   21910 non-null  object\n",
            " 1   text_banglish  21910 non-null  object\n",
            " 2   label          21910 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 513.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define the text preprocessing function ---\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # Convert to lowercase and ensure it's a string\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) # Remove punctuation, numbers, etc.\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# --- 2. Apply the function to create a 'cleaned_text' column ---\n",
        "df['cleaned_text'] = df['text_banglish'].apply(preprocess_text)\n",
        "\n",
        "# --- 3. Tokenize the cleaned text into lists of words ---\n",
        "df['tokenized_text'] = df['cleaned_text'].apply(lambda x: x.split())\n",
        "\n",
        "# --- 4. Safeguard: Remove any rows that became empty after cleaning ---\n",
        "original_rows = len(df)\n",
        "df = df[df['cleaned_text'] != ''].reset_index(drop=True)\n",
        "if original_rows > len(df):\n",
        "    print(f\"Removed {original_rows - len(df)} rows that were empty after preprocessing.\")\n",
        "\n",
        "# --- 5. Display the result to verify ---\n",
        "print(\"DataFrame after preprocessing and tokenization:\")\n",
        "print(df[['text_banglish', 'cleaned_text', 'tokenized_text', 'label']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY0Y1qesMYv9",
        "outputId": "fe606aa1-a892-4951-bd4d-5f81978ad885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 2 rows that were empty after preprocessing.\n",
            "DataFrame after preprocessing and tokenization:\n",
            "                                       text_banglish  \\\n",
            "0  manasika sharirikabhabe asustha klanta puro ji...   \n",
            "1  daya sathe thakuna atyanta dirgha apanake pada...   \n",
            "2  janatama sathe bhula loka kharapa jibana katiy...   \n",
            "3  anetibha imreji spikarera anusarana biraktikar...   \n",
            "4  anetibha imreji spikarera anusarana biraktikar...   \n",
            "\n",
            "                                        cleaned_text  \\\n",
            "0  manasika sharirikabhabe asustha klanta puro ji...   \n",
            "1  daya sathe thakuna atyanta dirgha apanake pada...   \n",
            "2  janatama sathe bhula loka kharapa jibana katiy...   \n",
            "3  anetibha imreji spikarera anusarana biraktikar...   \n",
            "4  anetibha imreji spikarera anusarana biraktikar...   \n",
            "\n",
            "                                      tokenized_text  label  \n",
            "0  [manasika, sharirikabhabe, asustha, klanta, pu...      1  \n",
            "1  [daya, sathe, thakuna, atyanta, dirgha, apanak...      1  \n",
            "2  [janatama, sathe, bhula, loka, kharapa, jibana...      1  \n",
            "3  [anetibha, imreji, spikarera, anusarana, birak...      1  \n",
            "4  [anetibha, imreji, spikarera, anusarana, birak...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate the tokenized sentences for training\n",
        "sentences = df['tokenized_text'].tolist()\n",
        "\n",
        "# --- Word2Vec Model Training ---\n",
        "# Define the model parameters based on your successful BiGRU script\n",
        "embedding_dim = 100  # Dimensionality of the word vectors\n",
        "window_size = 5      # Context window size\n",
        "min_word_count = 1   # Minimum word count to be included\n",
        "\n",
        "# sg=1 trains the Skip-Gram model, which often works well.\n",
        "print(\"Training Word2Vec model... (This might take a minute or two)\")\n",
        "w2v_model = Word2Vec(sentences=sentences,\n",
        "                     vector_size=embedding_dim,\n",
        "                     window=window_size,\n",
        "                     min_count=min_word_count,\n",
        "                     workers=4, # Use 4 CPU cores for training\n",
        "                     sg=1)\n",
        "\n",
        "print(\"Word2Vec model trained successfully.\")\n",
        "\n",
        "# --- Sanity Check ---\n",
        "# Check the vocabulary size\n",
        "vocab_size = len(w2v_model.wv.index_to_key)\n",
        "print(f\"\\nVocabulary size: {vocab_size}\")\n",
        "\n",
        "# Test the model by finding the most similar words to a sample word.\n",
        "# 'ami' ('I' in Bengali) is a common word and likely to be in the vocabulary.\n",
        "try:\n",
        "    sample_word = 'ami'\n",
        "    similar_words = w2v_model.wv.most_similar(sample_word)\n",
        "    print(f\"\\nWords most similar to '{sample_word}':\")\n",
        "    print(similar_words)\n",
        "except KeyError:\n",
        "    print(f\"\\nCould not perform similarity check. The word '{sample_word}' was not in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrXdxpNrNfEo",
        "outputId": "b667488d-8660-40ff-925b-54f3d41d5f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Word2Vec model... (This might take a minute or two)\n",
            "Word2Vec model trained successfully.\n",
            "\n",
            "Vocabulary size: 56826\n",
            "\n",
            "Words most similar to 'ami':\n",
            "[('intaraayakashanera', 0.9455628991127014), ('kishorakishorike', 0.9404802918434143), ('penapala', 0.9387423992156982), ('asamrakshita', 0.9377596974372864), ('brendana', 0.9375109076499939), ('skulakaleja', 0.9345955848693848), ('anugamidera', 0.9344710111618042), ('partiibhente', 0.933462381362915), ('ranadauna', 0.932837963104248), ('narbha', 0.9325851202011108)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Use Keras Tokenizer to convert texts to integer sequences ---\n",
        "# We use the cleaned text column, as it's a single string per row.\n",
        "texts = df['cleaned_text'].tolist()\n",
        "labels = df['label'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "# The vocab size for the Embedding layer is the number of unique words + 1 (for the 0 padding token)\n",
        "keras_vocab_size = len(word_index) + 1\n",
        "print(f\"Vocabulary size for Keras Tokenizer: {keras_vocab_size}\")\n",
        "\n",
        "\n",
        "# --- 2. Pad sequences to a uniform length ---\n",
        "# We'll use the 95th percentile length to avoid excessive padding due to a few very long texts.\n",
        "lengths = [len(s) for s in sequences]\n",
        "maxlen = int(np.percentile(lengths, 95))\n",
        "print(f\"Padding sequences to a max length of: {maxlen}\")\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
        "y = labels\n",
        "\n",
        "\n",
        "# --- 3. Create the embedding matrix from our Word2Vec model ---\n",
        "# This matrix will be used as the initial weights for the Keras Embedding layer.\n",
        "embedding_matrix = np.zeros((keras_vocab_size, embedding_dim)) # embedding_dim is 100\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_vector = w2v_model.wv[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(f\"Shape of the embedding matrix: {embedding_matrix.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Split data into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y) # stratify ensures similar class distribution\n",
        "\n",
        "print(f\"\\nTraining data shape (X_train): {X_train.shape}\")\n",
        "print(f\"Testing data shape (X_test): {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGn0vQzANlwX",
        "outputId": "da36e7d9-5042-4be4-b6c5-e2790def6dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size for Keras Tokenizer: 56827\n",
            "Padding sequences to a max length of: 192\n",
            "Shape of the embedding matrix: (56827, 100)\n",
            "\n",
            "Training data shape (X_train): (17526, 192)\n",
            "Testing data shape (X_test): (4382, 192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Build the GRU Model ---\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer (loaded with our Word2Vec weights)\n",
        "# We set trainable=False because the embeddings are already pre-trained.\n",
        "model.add(Embedding(input_dim=keras_vocab_size,\n",
        "                    output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=maxlen,\n",
        "                    trainable=False))\n",
        "\n",
        "# GRU Layer (This is the main change from your BiGRU script)\n",
        "model.add(GRU(units=64))\n",
        "\n",
        "# Dropout for regularization to help prevent overfitting\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# A standard fully-connected Dense layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Final Output Layer for binary classification (using sigmoid for probabilities)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# --- 2. Compile the Model ---\n",
        "# We configure the model for training with an optimizer, loss function, and metrics.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
        "\n",
        "\n",
        "# --- 3. Print Model Summary ---\n",
        "# This is very useful for your paper to show the model's structure and parameters.\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "b35XEhYYN-o2",
        "outputId": "83d5c936-566e-45e2-b8c4-faaa30381482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │     \u001b[38;5;34m5,682,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Set up Training Parameters ---\n",
        "print(\"Starting model training...\")\n",
        "\n",
        "# Add a callback for Early Stopping to prevent overfitting.\n",
        "# This will stop training if the validation loss doesn't improve for 3 consecutive epochs\n",
        "# and will restore the weights from the best epoch.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define number of epochs and batch size\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# --- 2. Train the Model and Measure Time ---\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test), # Evaluate on test data at the end of each epoch\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the training time\n",
        "training_time = end_time - start_time\n",
        "print(f\"\\nTraining finished in {training_time:.2f} seconds (approx {training_time/60:.1f} minutes).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3eEzCrROMB2",
        "outputId": "7724a996-6978-4f7d-b15f-57e923569dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 140ms/step - accuracy: 0.5201 - loss: 0.6914 - precision: 0.5706 - recall: 0.1436 - val_accuracy: 0.5087 - val_loss: 0.6852 - val_precision: 0.5045 - val_recall: 0.9886\n",
            "Epoch 2/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 136ms/step - accuracy: 0.5333 - loss: 0.6809 - precision: 0.5365 - recall: 0.5067 - val_accuracy: 0.5043 - val_loss: 0.6894 - val_precision: 0.5023 - val_recall: 0.9786\n",
            "Epoch 3/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 141ms/step - accuracy: 0.5384 - loss: 0.6766 - precision: 0.5497 - recall: 0.4226 - val_accuracy: 0.8517 - val_loss: 0.3670 - val_precision: 0.7945 - val_recall: 0.9489\n",
            "Epoch 4/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 135ms/step - accuracy: 0.8771 - loss: 0.3136 - precision: 0.8627 - recall: 0.8948 - val_accuracy: 0.8875 - val_loss: 0.2818 - val_precision: 0.8610 - val_recall: 0.9243\n",
            "Epoch 5/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 150ms/step - accuracy: 0.8950 - loss: 0.2708 - precision: 0.8807 - recall: 0.9147 - val_accuracy: 0.8984 - val_loss: 0.2572 - val_precision: 0.9068 - val_recall: 0.8882\n",
            "Epoch 6/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 135ms/step - accuracy: 0.9020 - loss: 0.2430 - precision: 0.8889 - recall: 0.9174 - val_accuracy: 0.8902 - val_loss: 0.2879 - val_precision: 0.8407 - val_recall: 0.9630\n",
            "Epoch 7/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 139ms/step - accuracy: 0.9060 - loss: 0.2306 - precision: 0.8962 - recall: 0.9184 - val_accuracy: 0.9096 - val_loss: 0.2293 - val_precision: 0.9063 - val_recall: 0.9138\n",
            "Epoch 8/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 138ms/step - accuracy: 0.9155 - loss: 0.2132 - precision: 0.9093 - recall: 0.9228 - val_accuracy: 0.9078 - val_loss: 0.2473 - val_precision: 0.9306 - val_recall: 0.8814\n",
            "Epoch 9/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 139ms/step - accuracy: 0.9201 - loss: 0.2000 - precision: 0.9133 - recall: 0.9266 - val_accuracy: 0.9105 - val_loss: 0.2330 - val_precision: 0.9072 - val_recall: 0.9147\n",
            "Epoch 10/20\n",
            "\u001b[1m548/548\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 136ms/step - accuracy: 0.9264 - loss: 0.1945 - precision: 0.9193 - recall: 0.9358 - val_accuracy: 0.9062 - val_loss: 0.2442 - val_precision: 0.9263 - val_recall: 0.8828\n",
            "\n",
            "Training finished in 778.66 seconds (approx 13.0 minutes).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# --- 1. Performance Evaluation on Test Set ---\n",
        "print(\"--- Final Performance Evaluation ---\")\n",
        "\n",
        "# Get model's prediction probabilities on the test set\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to binary class labels (0 or 1) using a 0.5 threshold\n",
        "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calculate and print the final metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "# Note: AUC is calculated on the prediction probabilities, not the binary predictions\n",
        "auc_roc = roc_auc_score(y_test, y_pred_probs)\n",
        "\n",
        "print(f\"\\nOverall Test Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(f\"AUC-ROC:   {auc_roc:.4f}\\n\")\n",
        "\n",
        "# Display a detailed classification report including precision and recall\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Depressed (0)', 'Depressed (1)']))\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "# Structure: [[True Negative, False Positive], [False Negative, True Positive]]\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Practical Time Complexity Analysis ---\n",
        "print(\"--- Time Complexity ---\")\n",
        "\n",
        "# We already have the training time from the previous step\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds (approx. {training_time/60:.1f} minutes)\")\n",
        "\n",
        "# Measure Inference Time on the entire test set\n",
        "inference_start_time = time.time()\n",
        "_ = model.predict(X_test)\n",
        "inference_end_time = time.time()\n",
        "\n",
        "total_inference_time = inference_end_time - inference_start_time\n",
        "avg_inference_time_per_sample = total_inference_time / len(X_test)\n",
        "\n",
        "print(f\"Total Inference Time for {len(X_test)} samples: {total_inference_time:.4f} seconds\")\n",
        "print(f\"Average Inference Time per Sample: {avg_inference_time_per_sample * 1000:.4f} milliseconds\\n\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Practical Space Complexity Analysis ---\n",
        "print(\"--- Space Complexity ---\")\n",
        "\n",
        "# Number of total parameters in the model\n",
        "total_params = model.count_params()\n",
        "print(f\"Total Model Parameters: {total_params:,}\")\n",
        "\n",
        "# Save the model to a file to check its size on disk\n",
        "model_filename = \"depression_detection_gru.keras\"\n",
        "model.save(model_filename)\n",
        "model_size_bytes = os.path.getsize(model_filename)\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Model Size on Disk: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHV5dTPjOSRI",
        "outputId": "8ee9163b-30d3-41bb-add9-6381e6afb4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Final Performance Evaluation ---\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step\n",
            "\n",
            "Overall Test Metrics:\n",
            "Accuracy:  0.9096\n",
            "F1 Score:  0.9100\n",
            "AUC-ROC:   0.9670\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Not Depressed (0)       0.91      0.91      0.91      2190\n",
            "    Depressed (1)       0.91      0.91      0.91      2192\n",
            "\n",
            "         accuracy                           0.91      4382\n",
            "        macro avg       0.91      0.91      0.91      4382\n",
            "     weighted avg       0.91      0.91      0.91      4382\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1983  207]\n",
            " [ 189 2003]]\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Time Complexity ---\n",
            "Total Training Time: 778.66 seconds (approx. 13.0 minutes)\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step\n",
            "Total Inference Time for 4382 samples: 3.6408 seconds\n",
            "Average Inference Time per Sample: 0.8309 milliseconds\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Space Complexity ---\n",
            "Total Model Parameters: 5,716,685\n",
            "Model Size on Disk: 22.10 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "Xaww93yFSWzG",
        "outputId": "ab72232c-ef00-4a22-c595-e0a95635c944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m5,682,700\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m31,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,784,657\u001b[0m (22.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,784,657</span> (22.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m33,985\u001b[0m (132.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,985</span> (132.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,682,700\u001b[0m (21.68 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,682,700</span> (21.68 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m67,972\u001b[0m (265.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,972</span> (265.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9OUJ7EYS7GS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}